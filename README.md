# Clinical Note Benchmark

This repository provides tools and scripts for benchmarking clinical note generation and evaluation using various models and prompts. It is designed to facilitate the generation, evaluation, and analysis of clinical notes, supporting reproducible research in medical NLP.

## Features
- **Note Generation**: Generate synthetic clinical notes using different models and prompts.
- **Evaluation**: Evaluate generated notes against reference standards using ROUGE metrics and AI-based evaluation.
- **Data Management**: Organize results, standards, and metadata for easy analysis.
- **Extensible**: Easily add new models, prompts, or evaluation methods.

## Directory Structure
```
clinical_note_benchmark/
├── generate.py                 # Script for generating clinical notes
├── evaluate.py                 # Script for evaluating generated notes
├── requester.py                # Model requester classes
├── utils.py                    # Utility functions
├── configs.yaml                # Configuration file
├── results/                    # Generated notes and evaluation results
├── standards/                  # Reference standard notes
├── augmented-clinical-notes/   # Source data and templates
├── prompts/                    # Prompt definitions
|__ set_standards.py            # Sets standards
```

## Getting Started

### Prerequisites
- Python 3.8+
- [pip](https://pip.pypa.io/en/stable/)
- (Optional) Virtual environment (recommended)

### Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/amandamarg/clinical_note_benchmark.git
   cd clinical_note_benchmark
   ```
2. (Optional) Create and activate a virtual environment:
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Usage
- **Generate Notes**:
  ```bash
  python generate.py
  ```
- **Set Standars**:
  ```bash
  python set_standards.py
  ```
- **Evaluate Notes**:
  ```bash
  python evaluate.py
  ```
- **Generate Plots**:
  ```bash
  python plot.py
  ```
- **Custom Runs**:
  Modify `IDXS`, model names, prompt names, or other args in the scripts as needed.

## Data
- **augmented-clinical-notes/augmented_notes_30K.jsonl**: Source conversations for note generation.
- **standards/**: Reference notes for evaluation.
- **results/**: Generated notes and evaluation outputs. Each subdirectory in `results/` is named for the corresponding idx in `augmented_notes_30K.jsonl` and contains text/json. The notes generated for a transcript at a particular idx using some model and prompt will be located in the file `gen_note.txt` under `results/{idx}/{model}/{prompt}/{timestamp}` along with the `eval_report.json` and `rouge_plot.png` for that particular generated note.
- **plots/**: Plots generated by `plot.py` script

## Extending
- Add new models or prompts by updating `requester.py` and relevant scripts.
- Add new evaluation metrics in `evaluate.py` or `utils.py`.

## Citation
If you use this repository in your research, please cite appropriately.

## License
MIT License

## Contact
For questions or contributions, please open an issue or contact the repository owner.
